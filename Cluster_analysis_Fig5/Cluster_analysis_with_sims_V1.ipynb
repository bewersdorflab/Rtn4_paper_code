{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190ed07-1780-4121-8deb-3a46b8829c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PYME.LMVis import VisGUI\n",
    "\n",
    "%gui wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd57ba8-362e-40b2-a8f4-7102fc0f7016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pymevis = VisGUI.ipython_pymevisualize()\n",
    "pipeline = pymevis.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149f5a7-31f4-4aa8-aafe-df07523d0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PYME.IO import tabular\n",
    "\n",
    "\n",
    "##### PARAMETERS to Change (see 'cluster_analysis_V1' function below for more detailed descriptions of parameters)\n",
    "\n",
    "min_dist = 10       # minimum distance between skeleton vertices in determining the vector that points from each vertex\n",
    "                    # to the closest vertex that's at least this distance away (helps make nice backbone vectors)\n",
    "max_dist = 50       # maximum distance between skeleton vertices. This helps speed up the KDTree query\n",
    "max_nn = 200        # maximum # of nearest neighbors on the skeleton before they are removed\n",
    "ball_r = 50         # radius used for query_ball_points\n",
    "Rtn4_chan = 'chan1' # channel name for Rtn4 data\n",
    "high_filt=100          # Threshold for max distance a Rtn4 point can be from mesh and still be included in analysis\n",
    "low_filt=-100          # Threshold for min distance a Rtn4 point can be from mesh and still be included in analysis\n",
    "min_size = 10      # currently not used\n",
    "max_size = 100     # currently not used\n",
    "rg_factor = 0.925  # Factor to adjust radius of gyration for simulations\n",
    "savedir = \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_analysis_V2\\\\updated_analysis_20230111\"\n",
    "ver = 'V1-adj'\n",
    "\n",
    "datadir = [\"K:\\\\4Pi_data\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Cell02_DBSCAN-2_ROI_all-clumped_DBSCAN-clusters_measureClusters3D.csv\",\n",
    "           \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20220930_data\\\\Cell04_ROI1_all-clumped_MeasureCluster3D_DBSCAN.csv\",\n",
    "           \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20221004_data\\\\Cell05_ROI4_all-clumped_measureClusters3D.csv\"]\n",
    "c_points = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Cell02_DBSCAN-2_ROI_all-clumped_DBSCAN-clusters.hdf\",\n",
    "            \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20220930_data\\\\Cell04_ROI1_all-clumped_DBSCAN_Clusters.hdf\",\n",
    "            \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20221004_data\\\\Cell05_ROI4_all-clumped_DBSCAN_Clusters.hdf\"]\n",
    "mesh_fn = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Cell02_DBSCAN-2_ROI_shrinkwrap.stl\",\n",
    "               \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20220930_data\\\\Cell04_ROI1_shrinkwrap_1.stl\",\n",
    "               \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20221004_data\\\\Cell05_ROI4_shrinkwrap.stl\"]\n",
    "skelly = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Cell02_DBSCAN-2_ROI_skeleton.stl\",\n",
    "          \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20220930_data\\\\Cell04_ROI1_skeleton.stl\",\n",
    "          \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20221004_data\\\\Cell05_ROI4_skeleton.stl\"]\n",
    "loc_info = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Cell02_DBSCAN-2_ROI_all-clumped_dbscanID_locs.csv\",\n",
    "            \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20220930_data\\\\Cell04_ROI1_all_clumped_dbscanID_locs.csv\",\n",
    "            \"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20221004_data\\\\Cell05_ROI4_all-clumped_dbscanID_locs.csv\"]\n",
    "\n",
    "d_name = ['20210907_Cell02','20220930_Cell04','20221004_Cell05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d03b65-6db0-4a93-9dc1-27ea3703db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For figure data:\n",
    "# datadir = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Old_results\\\\Cell02_oligomer_ROI2_DBSCAN.csv\"]\n",
    "# c_points = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Old_results\\\\Cell02_oligomer_ROI2_DBSCAN.hdf\"]\n",
    "# mesh_fn = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Old_results\\\\Cell02_oligomer_ROI2_shrinkwrap.stl\"]\n",
    "# skelly = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Old_results\\\\Cell02_oligomer_ROI2_skeleton_V2.stl\"]\n",
    "# loc_info = [\"K:\\\\4Pi_data\\\\Oligomer_analysis\\\\Oligomer_data\\\\20210907_data\\\\Old_results\\\\Cell02_mapped_clumped_ROI2_dbscanID_locs.csv\"]\n",
    "# ver = 'test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757148f-e65c-4214-8dfb-1a104da659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'min_dist': min_dist, 'max_dist': max_dist, 'max_nn': max_nn, 'ball_r': ball_r, 'Rtn4_chan': Rtn4_chan,\n",
    "              'high_filt': high_filt, 'low_filt': low_filt, 'min_size': min_size,\n",
    "              'max_size': max_size, 'savedir': savedir, 'ver': ver, 'datadir': datadir, 'c_points': c_points,\n",
    "              'mesh_fn': mesh_fn, 'skelly': skelly, 'loc_info': loc_info, 'd_name': d_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b21aab-04de-41e5-a917-606ac9c4ab2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PYME.IO import tabular\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# generate 3D Gaussian\n",
    "# points = np.random.randn(100,3)*100\n",
    "\n",
    "#######################################################################################################################\n",
    "# Define needed functions\n",
    "#######################################################################################################################\n",
    "\n",
    "def add_ds_from_Nx3(points, pipeline, pymevis, ds_name='points', color=None, normals=None):\n",
    "    \"\"\"\n",
    "    Quickly add points/normals to the pipeline and pymevis display.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {'x': points[:,0], 'y': points[:,1], 'z': points[:,2]}\n",
    "    \n",
    "    if color is not None:\n",
    "        d['c'] = color\n",
    "        \n",
    "    if normals is not None:\n",
    "        d['xn'] = normals[:,0]\n",
    "        d['yn'] = normals[:,1]\n",
    "        d['zn'] = normals[:,2]\n",
    "\n",
    "    # create tabular mappingFilter data source and add it to pymevis\n",
    "    pipeline.addDataSource(ds_name, tabular.mappingFilter(d))\n",
    "\n",
    "    # select this data source (optional, but helps support \"default behavior\")\n",
    "    pipeline.selectDataSource(ds_name)\n",
    "\n",
    "    # Add a pointcloud layer that displays data source named 'points' (the one we just added)\n",
    "    pymevis.add_pointcloud_layer(ds_name=ds_name)\n",
    "    \n",
    "    if normals is not None:\n",
    "        pymevis.glCanvas.layers[-1].display_normals=True\n",
    "        pymevis.glCanvas.layers[-1].normal_scaling=25.0\n",
    "\n",
    "def add_mesh_ds(vertices, param, pipeline, pymevis, param_name='new_param', ds_name='new_param', color=None, normals=None):\n",
    "    \"\"\"\n",
    "    Quickly add surface that has information about distance to another surface\n",
    "    \"\"\"\n",
    "    \n",
    "    #d = {'vertices': vertices, 'faces': faces, 'surf-surf-distance': surf_dist}\n",
    "    d = {'x': vertices[:,0], 'y': vertices[:,1], 'z': vertices[:,2]} \n",
    "\n",
    "    if color is not None:\n",
    "        d['c'] = color\n",
    "        \n",
    "    if normals is not None:\n",
    "        d['xn'] = normals[:,0]\n",
    "        d['yn'] = normals[:,1]\n",
    "        d['zn'] = normals[:,2]\n",
    "\n",
    "    # create tabular mappingFilter data source and add it to pymevis\n",
    "    out = tabular.MappingFilter(d)\n",
    "    out.addColumn(param_name, param)\n",
    "    pipeline.addDataSource(ds_name, out)                    \n",
    "\n",
    "    # select this data source (optional, but helps support \"default behavior\")\n",
    "    pipeline.selectDataSource(ds_name) \n",
    "        \n",
    "def save_snapshot(canvas, file_name=None):\n",
    "    if True:\n",
    "        pixel_size=None\n",
    "        \n",
    "        if file_name is None:\n",
    "            file_name = wx.FileSelector('Save current view as', wildcard=\"PNG files(*.png)|*.png\",\n",
    "                            flags=wx.FD_SAVE | wx.FD_OVERWRITE_PROMPT)\n",
    "        \n",
    "        if file_name:\n",
    "            snap = canvas.getIm(pixel_size, GL_RGB)\n",
    "            print(snap.dtype, snap.shape, snap.max())\n",
    "            if snap.ndim == 3:\n",
    "                img = PIL.Image.fromarray(snap.transpose(1, 0, 2))\n",
    "                #img = toimage(snap.transpose(1, 0, 2))\n",
    "            else:\n",
    "                img = PIL.Image.fromarray(snap.transpose())\n",
    "                #img = toimage(snap.transpose())\n",
    "            \n",
    "            img = img.transpose(PIL.Image.FLIP_TOP_BOTTOM)\n",
    "            \n",
    "            if not file_name.endswith('.png'):\n",
    "                img.save('{}.png'.format(file_name))\n",
    "            else:\n",
    "                img.save('{}'.format(file_name))\n",
    "\n",
    "def fullprint(*args, **kwargs):\n",
    "  from pprint import pprint\n",
    "  import numpy\n",
    "  opt = numpy.get_printoptions()\n",
    "  numpy.set_printoptions(threshold=numpy.inf)\n",
    "  pprint(*args, **kwargs)\n",
    "  numpy.set_printoptions(**opt)\n",
    "    \n",
    "def tan_circ_sim(rad, count, loc_prec, rg_factor, centers, surf_verts, skeleton, vecs_hat):\n",
    "    \"\"\"\n",
    "    Simulate clusters that are disc-shaped, tangent to the surface, and centered at the true cluster centers.\n",
    "    They also contain the same number of simulated localizations as the real cluster they are based on.\n",
    "    \n",
    "    Followed advice for determining plane for cicle from:\n",
    "    https://stackoverflow.com/questions/71160423/how-to-sample-points-in-3d-in-python-with-origin-and-normal-vector\n",
    "    \n",
    "    Also followed advice for uniform random distributions on a circle from here:\n",
    "    https://math.stackexchange.com/questions/1307287/random-uniformly-distributed-points-in-a-circle\n",
    "    \n",
    "    ------\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    rad (numpy array): radius of the simulated disc\n",
    "    \n",
    "    count (numpy array of ints): number of localizations to be simulated in each cluster\n",
    "    \n",
    "    -------\n",
    "    Output:\n",
    "    -------\n",
    "    \n",
    "    cs_points_all (numpy array): coordinates of points in the simulated clusters\n",
    "    \n",
    "    \"\"\"\n",
    "    from numpy import pi\n",
    "\n",
    "    # parameters for this simulation\n",
    "    #rad = radius # radius of points sampled in the circle on the plane (probably leave as 1 always)\n",
    "    max_r = rad * rg_factor\n",
    "    v_num = count # number of vectors desired (evenly spaced around the circle)\n",
    "    lp = loc_prec\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # equations for making the vectors\n",
    "    # rho = np.linspace(0, 2*np.pi, v_num)\n",
    "    # x = np.cos(rho) * r\n",
    "    # y = np.sin(rho) * r\n",
    "    # z = np.zeros(rho.shape)\n",
    "    # positions of simulated clusters\n",
    "    arot_pos = centers\n",
    "    # establish initial vectors\n",
    "    tree_surf = cKDTree(surf_verts)\n",
    "    mean_tree_skeleton = cKDTree(skeleton)\n",
    "    arot_surf_dist, arot_surf_ind = tree_surf.query(arot_pos, k=1)\n",
    "    arot_skel_dist, arot_skel_ind = mean_tree_skeleton.query(arot_pos, k=1)\n",
    "    arot_skel_vec = arot_pos - skeleton[arot_skel_ind]\n",
    "    arot_vecs = np.cross(vecs_hat[arot_skel_ind], arot_skel_vec)\n",
    "    arot_tub = vecs_hat[arot_skel_ind]\n",
    "\n",
    "    # Create vectors in a circle in the plane perpendicular to the vector that points from cluster center to skeleton\n",
    "    f_all = []\n",
    "    cs_points_sep = []\n",
    "    cs_points_tog = []\n",
    "    for bd in range(len(arot_pos)):\n",
    "        rho = rng.uniform(0, 2*np.pi, int(v_num[bd]))\n",
    "        uni = rng.uniform(0,1,int(v_num[bd]))\n",
    "        p1 = arot_pos[bd]\n",
    "        n = arot_skel_vec[bd] # vector normal to the plane\n",
    "        #pp = arot_vecs[bd] #vector orthogonal to normal\n",
    "        nabs = np.absolute(n)\n",
    "        indices = (-n).argsort()[:3]\n",
    "        v = np.zeros(3)\n",
    "        v[indices[1]] = -n[indices[0]]\n",
    "        v[indices[0]] = n[indices[1]]\n",
    "        v[indices[2]] = 0\n",
    "        u = np.cross(n,v)\n",
    "        #normalize vectors\n",
    "        v_norm = v/np.linalg.norm(v)\n",
    "        u_norm = u/np.linalg.norm(u)\n",
    "        f = []\n",
    "        cs_points = []\n",
    "        for be in range(int(v_num[bd])):\n",
    "            r = np.sqrt(uni[be])*max_r[bd]\n",
    "            # x = np.cos(rho[be]) * r\n",
    "            # y = np.sin(rho[be]) * r\n",
    "            # z = np.zeros(rho[be].shape)\n",
    "            \n",
    "            # Take the center point and add the uniform random location around the circle then add some localization precision\n",
    "            unc = [rng.normal(loc=0,scale=lp,size=1),rng.normal(loc=0,scale=lp,size=1),rng.normal(loc=0,scale=lp,size=1)]\n",
    "            circ_p = np.zeros(3)\n",
    "            for bf in range(len(circ_p)):\n",
    "                circ_p[bf] = p1[bf] + r * v_norm[bf] * np.cos(rho[be]) + r * u_norm[bf] * np.sin(rho[be]) + unc[bf]# + rng.normal(loc=0,scale=lp,size=1)\n",
    "            circ_vec = p1-circ_p\n",
    "            cs_points_tog.append(circ_p)\n",
    "            cs_points.append(circ_p)\n",
    "            f.append(circ_vec/np.linalg.norm(circ_vec))\n",
    "        #print(cs_points_tog)\n",
    "        cs_points = np.asarray(cs_points)\n",
    "        cs_points_sep.append(cs_points)\n",
    "        f = np.asarray(f)\n",
    "        f_all.append(f)\n",
    "    cs_points_tog = np.asarray(cs_points_tog)\n",
    "    cs_points_sep = np.asarray(cs_points_sep)\n",
    "    f_all = np.asarray(f_all)\n",
    "    # print('search here')\n",
    "    # fullprint(cs_points_tog)\n",
    "    add_ds_from_Nx3(cs_points_tog, pipeline, pymevis, 'Disc-sim-clusters', normals=None)\n",
    "    \n",
    "    return cs_points_sep, cs_points_tog\n",
    "\n",
    "def tan_line_sim(rad, count, loc_prec, rg_factor, centers, surf_verts, skeleton, vecs_hat):\n",
    "    \"\"\"\n",
    "    Simulate clusters that are linearly-shaped, tangent to the surface, and centered at the true cluster centers.\n",
    "    They also contain the same number of simulated localizations as the real cluster they are based on, and the\n",
    "    same localization precision. The difference between these simulated clusters and the circular simulated\n",
    "    clusters, is the angle of each point in the circular clusters is randomized whereas all the points\n",
    "    in the linear clusters have the same angle. The radius varies for both.\n",
    "    \n",
    "    Followed advice for determining plane for cicle from:\n",
    "    https://stackoverflow.com/questions/71160423/how-to-sample-points-in-3d-in-python-with-origin-and-normal-vector\n",
    "    \n",
    "    Also followed advice for uniform random distributions on a circle from here:\n",
    "    https://math.stackexchange.com/questions/1307287/random-uniformly-distributed-points-in-a-circle\n",
    "    \n",
    "    ------\n",
    "    Input:\n",
    "    ------\n",
    "    \n",
    "    rad (numpy array): radius of the simulated disc\n",
    "    \n",
    "    count (numpy array of ints): number of localizations to be simulated in each cluster\n",
    "    \n",
    "    -------\n",
    "    Output:\n",
    "    -------\n",
    "    \n",
    "    cs_points_all (numpy array): coordinates of points in the simulated clusters\n",
    "    \n",
    "    \"\"\"\n",
    "    from numpy import pi\n",
    "\n",
    "    # parameters for this simulation\n",
    "    #rad = radius # radius of points sampled in the circle on the plane (probably leave as 1 always)\n",
    "    max_r = rad * rg_factor\n",
    "    v_num = count # number of vectors desired (evenly spaced around the circle)\n",
    "    lp = loc_prec\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # equations for making the vectors\n",
    "    # rho = np.linspace(0, 2*np.pi, v_num)\n",
    "    # x = np.cos(rho) * r\n",
    "    # y = np.sin(rho) * r\n",
    "    # z = np.zeros(rho.shape)\n",
    "    # positions of simulated clusters\n",
    "    arot_pos = centers\n",
    "    # establish initial vectors\n",
    "    tree_surf = cKDTree(surf_verts)\n",
    "    mean_tree_skeleton = cKDTree(skeleton)\n",
    "    arot_surf_dist, arot_surf_ind = tree_surf.query(arot_pos, k=1)\n",
    "    arot_skel_dist, arot_skel_ind = mean_tree_skeleton.query(arot_pos, k=1)\n",
    "    arot_skel_vec = arot_pos - skeleton[arot_skel_ind]\n",
    "    arot_vecs = np.cross(vecs_hat[arot_skel_ind], arot_skel_vec)\n",
    "    arot_tub = vecs_hat[arot_skel_ind]\n",
    "\n",
    "    # Create vectors in a circle in the plane perpendicular to the vector that points from cluster center to skeleton\n",
    "    f_all = []\n",
    "    ls_points_sep = []\n",
    "    ls_points_tog = []\n",
    "    for bd in range(len(arot_pos)):\n",
    "        rho = rng.uniform(0, 2*np.pi, 1) * np.ones(int(v_num[bd]))\n",
    "        uni = rng.uniform(-1,1,int(v_num[bd]))\n",
    "        p1 = arot_pos[bd]\n",
    "        n = arot_skel_vec[bd] # vector normal to the plane\n",
    "        #pp = arot_vecs[bd] #vector orthogonal to normal\n",
    "        nabs = np.absolute(n)\n",
    "        indices = (-n).argsort()[:3]\n",
    "        v = np.zeros(3)\n",
    "        v[indices[1]] = -n[indices[0]]\n",
    "        v[indices[0]] = n[indices[1]]\n",
    "        v[indices[2]] = 0\n",
    "        u = np.cross(n,v)\n",
    "        #normalize vectors\n",
    "        v_norm = v/np.linalg.norm(v)\n",
    "        u_norm = u/np.linalg.norm(u)\n",
    "        f = []\n",
    "        ls_points = []\n",
    "        for be in range(int(v_num[bd])):\n",
    "            # if uni[be] < 0:\n",
    "            #     r = np.sqrt(np.absolute(uni[be]))*max_r[bd]*(-1)\n",
    "            # else:\n",
    "            #     r = np.sqrt(uni[be])*max_r[bd]\n",
    "            r = uni[be] * max_r[bd]\n",
    "            \n",
    "            # Take the center point and add the uniform random location around the circle then add some localization precision\n",
    "            unc = [rng.normal(loc=0,scale=lp,size=1),rng.normal(loc=0,scale=lp,size=1),rng.normal(loc=0,scale=lp,size=1)]\n",
    "            circ_p = np.zeros(3)\n",
    "            for bf in range(len(circ_p)):\n",
    "                circ_p[bf] = p1[bf] + r * v_norm[bf] * np.cos(rho[be]) + r * u_norm[bf] * np.sin(rho[be]) + unc[bf]# + rng.normal(loc=0,scale=lp,size=1)\n",
    "            circ_vec = p1-circ_p\n",
    "            ls_points_tog.append(circ_p)\n",
    "            ls_points.append(circ_p)\n",
    "            f.append(circ_vec/np.linalg.norm(circ_vec))\n",
    "        #print(cs_points_tog)\n",
    "        ls_points = np.asarray(ls_points)\n",
    "        ls_points_sep.append(ls_points)\n",
    "        f = np.asarray(f)\n",
    "        f_all.append(f)\n",
    "    ls_points_tog = np.asarray(ls_points_tog)\n",
    "    ls_points_sep = np.asarray(ls_points_sep)\n",
    "    f_all = np.asarray(f_all)\n",
    "    \n",
    "    add_ds_from_Nx3(ls_points_tog, pipeline, pymevis, 'Line-sim-clusters', normals=None)\n",
    "    \n",
    "    return ls_points_sep, ls_points_tog\n",
    "    \n",
    "def proj_u_onto_plane(u, n):\n",
    "    ''' \n",
    "    Project vector u onto plane that is orthogonal to vector n\n",
    "    ----------------------------------------------------------\n",
    "    Input:\n",
    "        u: vector to be projected onto plane\n",
    "        n: vector normal to the plane\n",
    "    Output:\n",
    "        u_p: projection of vector u onto plane with normal n\n",
    "    '''\n",
    "    n_norm = np.sqrt(sum(n**2))\n",
    "    proj_of_u_on_n = (np.dot(u, n)/n_norm**2)*n\n",
    "    u_p = u - proj_of_u_on_n\n",
    "    return u_p\n",
    "\n",
    "def vector_projection(cp, s):\n",
    "    '''\n",
    "    Project vector cp onto vector s\n",
    "    -------------------------------\n",
    "    Input:\n",
    "        cp: projection of cluster vectors c onto plane normal to r\n",
    "        s: used skeleton vectors\n",
    "    Output:\n",
    "        a: component of cp that is parallel to s (vector projection of cp onto s)\n",
    "        \n",
    "    Followed: https://www.geeksforgeeks.org/vector-projection-using-python/\n",
    "    '''\n",
    "    s_norm = np.sqrt(sum(s**2))\n",
    "    a = (np.dot(cp, s)/s_norm**2)*s\n",
    "    return a\n",
    "    \n",
    "def angle_btwn_vectors(b, cp):\n",
    "    '''\n",
    "    Calculate angle(alpha) between vector b and vector cp\n",
    "    ----------------------------------------------\n",
    "    Input:\n",
    "        cp: projection of cluster vectors c onto plane normal to r\n",
    "        b: the component of cp orthogonal to the skeleton axis\n",
    "    Output:\n",
    "        alpha: angle between vector b and vector cp\n",
    "    '''\n",
    "    b_norm = np.sqrt(sum(b**2))\n",
    "    cp_norm = np.sqrt(sum(cp**2))\n",
    "    alpha = np.absolute(np.arccos(np.dot(b,cp)/(b_norm*cp_norm))) #absolute value is technically correct but not actually necessary because angle between b and cp is range[-90 to +90]\n",
    "    return alpha\n",
    "\n",
    "def handedness(r, b, a):\n",
    "    '''\n",
    "    Calculate handedness of cluster vectors\n",
    "    ---------------------------------------\n",
    "    Input:\n",
    "        r: vector from the centers of clusters to the closest part of the skeleton \n",
    "            (vector rejections of centers to skeleton points as these aren't always \n",
    "            perpindicular but the rejections are)\n",
    "        b: the component of cp orthogonal to the skeleton axis\n",
    "        a: component of cp that is parallel to s (vector projection of cp onto s)\n",
    "    Output:\n",
    "        h (-1 or 1): reflecting left or right handedness of cluster major axes on the\n",
    "            tubule\n",
    "    '''\n",
    "    rxb_norm = np.sqrt(sum((np.cross(r,b))**2))\n",
    "    a_norm = np.sqrt(sum(a**2))\n",
    "    sign = np.dot(np.cross(r,b),a)/(rxb_norm*a_norm)\n",
    "    return sign\n",
    "\n",
    "def cluster_angles(r, c, s):\n",
    "    '''\n",
    "    Input: these 3 vectors should all be the same size\n",
    "        r: vector from the centers of clusters to the closest part of the skeleton \n",
    "            (vector rejections of centers to skeleton points as these aren't always \n",
    "            perpendicular but the rejections are)\n",
    "        c: cluster major axis vectors\n",
    "        s: used skeleton vectors\n",
    "        \n",
    "    Output:\n",
    "        psi: angle between the vector projection of c onto a plane positioned on the \n",
    "            surface and normal to r and vector b which the component of c orthogonal \n",
    "            to the skeleton axis\n",
    "    '''\n",
    "    cp = proj_u_onto_plane(c, r)\n",
    "    a = vector_projection(cp, s)\n",
    "    b = cp - a\n",
    "    alpha = angle_btwn_vectors(cp,b)\n",
    "    psi = angle_btwn_vectors(cp,s)\n",
    "    sign = handedness(r, b, a)\n",
    "    signed_alpha = alpha*sign\n",
    "    signed_psi = psi*sign\n",
    "    return signed_alpha, signed_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c164eb-d629-49ff-938f-eeaec23a97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_analysis_V1(params):\n",
    "    '''\n",
    "    This function does a number of things:\n",
    "    \n",
    "    - Calculate the angle between cluster vectors projected onto the surface and \n",
    "        that vectors component that is orthogonal to the tubule axis (angle psi)\n",
    "    \n",
    "    - Simulate circular and linear clusters and compare their anisotropies to the \n",
    "        real data\n",
    "    \n",
    "    Input:\n",
    "    \n",
    "    - param_list: Python dictionary of all required parameters (created above in \n",
    "        the parameter cell)\n",
    "        \n",
    "        - min_dist (int or float): minimum distance between skeleton vertices. \n",
    "            Used for calculating skeleton vectors. They point from each skeleton \n",
    "            point to the closest point that's at least this distance away\n",
    "            \n",
    "        - max_dist (int or float): maximum distance between skeleton vertices. \n",
    "            This helps speed up the KDTree query\n",
    "        \n",
    "        - max_nn (int): maximum # of nearest neighbors on the skeleton before \n",
    "            they are removed. This effectively removes sheets from the skeleton\n",
    "            which we intentionally exclude from analyses.\n",
    "            \n",
    "        - ball_r (int or float): radius used for query_ball_points. Used for\n",
    "            trimming the skeleton along with max_nn.\n",
    "            \n",
    "        - Rtn4_chan ('chan0' or 'chan1'): which channel of the two-color 4Pi data \n",
    "            is the one corresponding to the Rtn4 localizations.\n",
    "            \n",
    "        - high_filt (int or float): Threshold for max distance a Rtn4 point can \n",
    "            be from mesh surface and still be included in analysis.\n",
    "            \n",
    "        - low_filt (int or float): Threshold for min distance a Rtn4 point can be \n",
    "            from mesh surface and still be included in analysis.\n",
    "            \n",
    "        - min_size (int): Threshold for minimum # of localizations a cluster must \n",
    "            possess to be included in the anlaysis.\n",
    "            \n",
    "        - max_size (int): Threshold for maximum # of localizations a cluster must \n",
    "            possess to be included in the anlaysis.\n",
    "            \n",
    "        - savedir (str): File path where all file outputs will be saved.\n",
    "            \n",
    "        - ver (str): This string is included in every saved file name to make \n",
    "            sure older files are not overwritten if that is not desired.\n",
    "            \n",
    "        - datadir (list): list of file paths to .csv files of each dataset containing\n",
    "            the results for MeasureClusters3D performed on DBSCAN segmetned clusters\n",
    "            \n",
    "        - c_points (list): list of file paths to .hdf files containing the DBSCAN\n",
    "            segmented Rtn4 clusters.\n",
    "            \n",
    "        - mesh_fn (list): list of file paths to .stl files containing the \n",
    "            shrink-wrapped surfaces of each dataset.\n",
    "            \n",
    "        - skelly (list): list of file paths to .stl files containing the\n",
    "            skeletons of each dataset.\n",
    "            \n",
    "        - loc_info (list): list of file paths to .csv files containing all info\n",
    "            about each localization, including the cluster they belong to.\n",
    "            \n",
    "        - d_name (list): list of strings that are the names of the datasets\n",
    "        \n",
    "    Output:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read data except for axes which are more involved\n",
    "    clust_data = []\n",
    "    cluster_output_all = []\n",
    "    circ_sims_all = []\n",
    "    line_sims_all = []\n",
    "    adj_sims_all = []\n",
    "    for dt in range(len(params['datadir'])):\n",
    "        c_data = np.genfromtxt(params['datadir'][dt], delimiter = ',')\n",
    "        l_data = np.genfromtxt(loc_info[dt], delimiter = ',')\n",
    "        clust = {\"count\": c_data[:,0], \"x\": c_data[:,1], \"y\": c_data[:,2], \"z\": c_data[:,3], \"gyrationRadius\": c_data[:,4], \n",
    "                \"median_abs_deviation\": c_data[:,5], \"sigma0\": c_data[:,9], \"sigma1\": c_data[:,10], \"sigma2\": c_data[:,11], \n",
    "                \"sigma_x\": c_data[:,12], \"sigma_y\": c_data[:,13], \"sigma_z\": c_data[:,14], \"anisotropy\": c_data[:,15], \n",
    "                \"theta\": c_data[:,16], \"phi\": c_data[:,17], \"loc_prec\": l_data[:,12]}\n",
    "\n",
    "        #making save-folder if it doesn't exist (DON'T TOUCH THE NEXT 5 LINES)\n",
    "        from pathlib import Path\n",
    "        import os\n",
    "        p = Path(params['savedir'])\n",
    "        if not p.exists():\n",
    "            os.mkdir(params['savedir'])\n",
    "        workdir = os.getcwd()\n",
    "\n",
    "        import pandas as pd\n",
    "        import copy\n",
    "\n",
    "        if 'check1' in locals():\n",
    "            del check1\n",
    "        if 'check2' in locals():\n",
    "            del check2\n",
    "        if 'x' in locals():\n",
    "            del x\n",
    "        if 'y' in locals():\n",
    "            del y\n",
    "        if 'z' in locals():\n",
    "            del z\n",
    "\n",
    "        # Initialize variables\n",
    "        csv_file = pd.read_csv(params['datadir'][dt], sep=',', header=None)\n",
    "        clusto = {\"axis0\": np.copy(csv_file[6][1:].values), \"axis1\": np.copy(csv_file[7][1:].values), \n",
    "                  \"axis2\": np.copy(csv_file[8][1:].values)}\n",
    "        clust['axis0'] = np.zeros((len(clust['count']),3))\n",
    "        clust['axis1'] = np.zeros((len(clust['count']),3))\n",
    "        clust['axis2'] = np.zeros((len(clust['count']),3))\n",
    "\n",
    "\n",
    "        # Convert 'axis0', 'axis1', and 'axis2' values to floats from strings\n",
    "        # axis0\n",
    "        for a in range(len(clusto[\"axis0\"])):\n",
    "            #print('a:',a)\n",
    "            if a > 0:\n",
    "                del x,y,z,check1,check2\n",
    "            if clusto['axis0'][a][2:4] == '[ ': \n",
    "                for b in range(1,len(clusto['axis0'][a])):\n",
    "                    if clusto['axis0'][a][b] == ' ' and 'check1' not in locals():\n",
    "                        check1 = b\n",
    "                    elif clusto['axis0'][a][b] == ' ' and 'check2' not in locals():\n",
    "                        x = float(clusto['axis0'][a][3:b])\n",
    "                        check2 = b\n",
    "                    elif clusto['axis0'][a][b] == ' ' and clusto['axis0'][a][b-1] != ' ' and 'check2' in locals() and 'y' not in locals():             \n",
    "                        y = float(clusto['axis0'][a][check2:b])\n",
    "                        z = float(clusto['axis0'][a][b:-2])\n",
    "            elif clusto['axis0'][a][2] == '[' and clusto['axis0'][a][3] != ' ': \n",
    "                for b in range(1,len(clusto['axis0'][a])):\n",
    "                    if clusto['axis0'][a][b] == ' ' and 'check1' not in locals():\n",
    "                        check1 = b\n",
    "                        x = float(clusto['axis0'][a][3:b])\n",
    "                    elif clusto['axis0'][a][b] == ' ' and clusto['axis0'][a][b-1] != ' ' and 'check2' not in locals() and 'y' not in locals():# 'check2' not in locals():\n",
    "                        y = float(clusto['axis0'][a][check1:b])\n",
    "                        z = float(clusto['axis0'][a][b:-2])\n",
    "                        check2 = b\n",
    "            clust['axis0'][a] = np.array([x, y, z])\n",
    "        del check1, check2, x, y, z, a, b\n",
    "\n",
    "        # axis1\n",
    "        for c in range(len(clusto[\"axis1\"])):\n",
    "            if c > 0:\n",
    "                del x,y,z,check1,check2\n",
    "            if clusto['axis1'][c][2:4] == '[ ': \n",
    "                for d in range(1,len(clusto['axis1'][c])):\n",
    "                    if clusto['axis1'][c][d] == ' ' and 'check1' not in locals():\n",
    "                        check1 = d\n",
    "                    elif clusto['axis1'][c][d] == ' ' and 'check2' not in locals():\n",
    "                        x = float(clusto['axis1'][c][3:d])\n",
    "                        check2 = d\n",
    "                    elif clusto['axis1'][c][d] == ' ' and clusto['axis1'][c][d-1] != ' ' and 'check2' in locals() and 'y' not in locals():             \n",
    "                        y = float(clusto['axis1'][c][check2:d])\n",
    "                        z = float(clusto['axis1'][c][d:-2])\n",
    "            elif clusto['axis1'][c][2] == '[' and clusto['axis1'][c][3] != ' ': \n",
    "                for d in range(1,len(clusto['axis1'][c])):\n",
    "                    if clusto['axis1'][c][d] == ' ' and 'check1' not in locals():\n",
    "                        check1 = d\n",
    "                        x = float(clusto['axis1'][c][3:d])\n",
    "                    elif clusto['axis1'][c][d] == ' ' and clusto['axis1'][c][d-1] != ' ' and 'check2' not in locals() and 'y' not in locals():# 'check2' not in locals():\n",
    "                        y = float(clusto['axis1'][c][check1:d])\n",
    "                        z = float(clusto['axis1'][c][d:-2])\n",
    "                        check2 = d\n",
    "            clust['axis1'][c] = np.array([x, y, z])\n",
    "        del check1, check2, x, y, z, c, d\n",
    "\n",
    "        # axis2\n",
    "        for e in range(len(clusto[\"axis2\"])):\n",
    "            if e > 0:\n",
    "                del x,y,z,check1,check2\n",
    "            if clusto['axis2'][e][2:4] == '[ ':\n",
    "                for f in range(1,len(clusto['axis2'][e])):\n",
    "                    if clusto['axis2'][e][f] == ' ' and 'check1' not in locals():\n",
    "                        check1 = f\n",
    "                    elif clusto['axis2'][e][f] == ' ' and 'check2' not in locals():\n",
    "                        x = float(clusto['axis2'][e][3:f])\n",
    "                        check2 = f\n",
    "                    elif clusto['axis2'][e][f] == ' ' and clusto['axis2'][e][f-1] != ' ' and 'check2' in locals() and 'y' not in locals():             \n",
    "                        y = float(clusto['axis2'][e][check2:f])\n",
    "                        z = float(clusto['axis2'][e][f:-2])\n",
    "            elif clusto['axis2'][e][2] == '[' and clusto['axis2'][e][3] != ' ':\n",
    "                for f in range(1,len(clusto['axis2'][e])):\n",
    "                    if clusto['axis2'][e][f] == ' ' and 'check1' not in locals():\n",
    "                        check1 = f\n",
    "                        x = float(clusto['axis2'][e][3:f])\n",
    "                    elif clusto['axis2'][e][f] == ' ' and clusto['axis2'][e][f-1] != ' ' and 'check2' not in locals() and 'y' not in locals():# 'check2' not in locals():\n",
    "                        y = float(clusto['axis2'][e][check1:f])\n",
    "                        z = float(clusto['axis2'][e][f:-2])\n",
    "                        check2 = f\n",
    "            clust['axis2'][e] = np.array([x, y, z])\n",
    "\n",
    "        clust_data.append(clust)\n",
    "        del clust, c_data\n",
    "\n",
    "        # Load and display points\n",
    "        pymevis.OpenFile(params['c_points'][dt])\n",
    "\n",
    "        # define mesh sdf\n",
    "        from PYME.experimental.isosurface import distance_to_mesh\n",
    "\n",
    "        mesh_sdf = lambda pts: distance_to_mesh(pts.T, mesh)\n",
    "\n",
    "        # Load and display mesh\n",
    "        from PYME.experimental._triangle_mesh import TriangleMesh\n",
    "        from PYME.LMVis.layers.mesh import TriangleRenderLayer\n",
    "\n",
    "        mesh = TriangleMesh.from_stl(params['mesh_fn'][dt])\n",
    "\n",
    "        mesh_name = pipeline.new_ds_name('surf')\n",
    "        pipeline.recipe.namespace[mesh_name] = mesh\n",
    "        layer = TriangleRenderLayer(pipeline, dsname=mesh_name, method='wireframe', cmap = 'SolidCyan')\n",
    "        pymevis.add_layer(layer)\n",
    "\n",
    "        # grab the bounding box\n",
    "        mesh_bbox = pymevis.glCanvas.layers[-1]._bbox\n",
    "\n",
    "        # Load and display skeleton (this is replacing the skeleton creation within the script approach because\n",
    "        # it takes so long to make the skeleton. Better to do it once and then load it in for analyses\n",
    "\n",
    "        from PYME.experimental._triangle_mesh import TriangleMesh\n",
    "        from PYME.LMVis.layers.mesh import TriangleRenderLayer\n",
    "\n",
    "        mesh = TriangleMesh.from_stl(params['skelly'][dt])\n",
    "\n",
    "        mesh_name = pipeline.new_ds_name('skeleton')\n",
    "        pipeline.recipe.namespace[mesh_name] = mesh\n",
    "        layer = TriangleRenderLayer(pipeline, dsname=mesh_name, method='wireframe', cmap = 'SolidMagenta')\n",
    "        pymevis.add_layer(layer)\n",
    "\n",
    "        # grab the bounding box\n",
    "        mesh_bbox = pymevis.glCanvas.layers[-1]._bbox\n",
    "\n",
    "        # Import ExtractTableChannel Recipe\n",
    "        from PYME.recipes.localisations import ExtractTableChannel\n",
    "        recipe = pipeline.recipe\n",
    "        # Extract Rtn4 points as 'Rtn4' DataSource\n",
    "        recipe.add_modules_and_execute([ExtractTableChannel(recipe, inputName = 'filtered_localizations', outputName = 'Rtn4',\n",
    "                                                            channel = params['Rtn4_chan'])])\n",
    "\n",
    "        # Select simulated DataSource\n",
    "        pipeline.selectDataSource('Rtn4')\n",
    "\n",
    "        # Calculate distance from simulated points to surface\n",
    "        from PYME.recipes.surface_fitting import DistanceToMesh\n",
    "        recipe = pipeline.recipe\n",
    "        recipe.add_modules_and_execute([DistanceToMesh(recipe, input_mesh = 'surf0', input_points = 'Rtn4', output = 'Rtn4_filt')])\n",
    "\n",
    "        # Select simulated datasource that now has the distance_to_surf0 values associated with it\n",
    "        pipeline.selectDataSource('Rtn4_filt')\n",
    "\n",
    "        # Dictionary with info about what filter to use and what values\n",
    "        filt_dict_rtn = {'distance_to_surf0': [params['low_filt'],params['high_filt']]}\n",
    "\n",
    "        # Use PYME FilterTable to filter by distance to mesh (SDF) using low_filt and high_filt variables\n",
    "        from PYME.recipes.tablefilters import FilterTable\n",
    "        recipe.add_modules_and_execute([FilterTable(recipe, inputName = 'Rtn4_filt', filters = filt_dict_rtn, \n",
    "                                                    outputName = 'sdf_filtered_Rtn4')])\n",
    "\n",
    "        # Select the simulate points that passed the SDF filter above\n",
    "        pipeline.selectDataSource('sdf_filtered_Rtn4')\n",
    "\n",
    "        # Extract x,y,z coordinates for simulated points\n",
    "        x_rtn = pipeline['x']\n",
    "        y_rtn = pipeline['y']\n",
    "        z_rtn = pipeline['z']\n",
    "\n",
    "        # Merge x,y,z coordinates into one variable 'points_rtn'\n",
    "        points_rtn = np.c_[x_rtn.ravel(),y_rtn.ravel(),z_rtn.ravel()]\n",
    "\n",
    "        # Import cKDTree function from SciPy and use it on the skeleton\n",
    "        pipeline.selectDataSource('skeleton0')\n",
    "        d_skelly = pipeline.selectedDataSource\n",
    "        orig_skeleton = d_skelly._vertices['position'][d_skelly._vertices['halfedge']!=-1]\n",
    "        from scipy.spatial import cKDTree\n",
    "        tree_orig = cKDTree(orig_skeleton)\n",
    "\n",
    "        # Move each vertex to the average position of all of its neighbors within a 'ball_r' radius\n",
    "        # Also trim out vertices that have a # of nearest neighbors > than max_nn (these are the q-tip looking spots)\n",
    "        # This doesn't make the problem areas better, it just cuts them out entirely\n",
    "        from collections import OrderedDict\n",
    "        skeleton = orig_skeleton\n",
    "        bad_inds = []\n",
    "        a_ind = tree_orig.query_ball_point(skeleton, params['ball_r'])\n",
    "        for trim in range(len(a_ind)):\n",
    "            skeleton[trim] = np.mean(skeleton[a_ind[trim]],axis=0)\n",
    "            if len(a_ind[trim]) > params['max_nn']:\n",
    "                bad_inds.append(a_ind[trim])\n",
    "\n",
    "        flat_bad_inds = [element for sublist in bad_inds for element in sublist]\n",
    "        flat_unique = list(OrderedDict.fromkeys(flat_bad_inds))\n",
    "        skeleton = np.delete(skeleton, flat_unique, axis=0)\n",
    "\n",
    "        # Make KDTree based on the mean skeleton\n",
    "        mean_tree_skeleton = cKDTree(skeleton)\n",
    "\n",
    "        # Find the closest neighbor that is at least 'min_dist' away from point being queried.\n",
    "        nneigh_ind = np.zeros([len(skeleton),1])\n",
    "        nneigh_dist = np.zeros([len(skeleton),1])\n",
    "        for aa in range(len(skeleton)):\n",
    "            dist, ind = mean_tree_skeleton.query(skeleton[aa], k=len(skeleton), distance_upper_bound=params['max_dist']*2)\n",
    "            for ab in range(len(dist)):\n",
    "                if dist[ab] > params['min_dist']:\n",
    "                    nneigh_ind[aa] = ind[ab]\n",
    "                    nneigh_dist[aa] = dist[ab]\n",
    "                    break\n",
    "\n",
    "        # Calculate vectors pointing from each point to its nearest neighbor that is at least 'min_dist' away.\n",
    "        skel_vecs = np.zeros([len(skeleton),3])\n",
    "        bad_skel = []\n",
    "        for ac in range(len(skeleton)):\n",
    "            if nneigh_ind[ac] >= len(skeleton):\n",
    "                bad_skel.append(ac)\n",
    "            else:\n",
    "                vec = skeleton[ac] - skeleton[int(nneigh_ind[ac][0])]\n",
    "                skel_vecs[int(ac)] = vec\n",
    "        bad_skel = np.asarray(bad_skel)\n",
    "        skeleton = np.delete(skeleton, bad_skel, axis=0)\n",
    "        nneigh_dist = np.delete(nneigh_dist, bad_skel, axis=0)\n",
    "        nneigh_ind = np.delete(nneigh_ind, bad_skel, axis=0)\n",
    "        skel_vecs = np.delete(skel_vecs, bad_skel, axis=0)\n",
    "\n",
    "        del mean_tree_skeleton\n",
    "        mean_tree_skeleton = cKDTree(skeleton)\n",
    "\n",
    "        # Normalize\n",
    "        skel_dist_mag = np.c_[nneigh_dist.ravel(),nneigh_dist.ravel(),nneigh_dist.ravel()]\n",
    "        vecs_hat = skel_vecs/skel_dist_mag\n",
    "\n",
    "        # Add those vectors to the skeleton in the GUI\n",
    "        # add_ds_from_Nx3(skeleton, pipeline, pymevis, ds_name=f'nn-mean-skel-{min_dist}nm', normals=vecs_hat)\n",
    "\n",
    "        # skel_dist = distance from each Rtn4 point to closest skeleton point; skel_ind = index of the closest point in 'skeleton'\n",
    "        skel_dist_rtn, skel_ind_rtn = mean_tree_skeleton.query(points_rtn, k=1)\n",
    "\n",
    "        ############################# Plot angles of centers of clusters to skeleton #############################\n",
    "\n",
    "        # map centers of clusters to 'centers' as xyz coordinates\n",
    "        centers = np.zeros([len(clust_data[dt]['x']),3])\n",
    "        for g in range(len(clust_data[dt]['x'])):\n",
    "            centers[g] = [clust_data[dt]['x'][g],clust_data[dt]['y'][g],clust_data[dt]['z'][g]]\n",
    "\n",
    "        # determine closest skeleton point to each cluster center\n",
    "        skel_dist_cent, skel_ind_cent = mean_tree_skeleton.query(centers, k=1)\n",
    "\n",
    "        # vector from 1st nearest neighbor to Rtn cluster center\n",
    "        v_cent = skeleton[skel_ind_cent]-centers\n",
    "\n",
    "        # make skel_dist an [n, 3] array to match v_cent\n",
    "        skel_dist_mag_cent = np.c_[skel_dist_cent.ravel(),skel_dist_cent.ravel(),skel_dist_cent.ravel()]\n",
    "\n",
    "        # normalized/unit vector pointing from skeleton to Rtn cluster center point (divide each part of the vector by magnitidue of vector)\n",
    "        vp_cent = v_cent/skel_dist_mag_cent\n",
    "\n",
    "        # Calculate vector rejections to use for angle analysis\n",
    "        #vecs_hat_ind = vecs_hat[skel_ind_cent] #only relevant skeleton vectors\n",
    "        skel_vecs_used = vecs_hat[skel_ind_cent]\n",
    "        v_cent = []\n",
    "        v_cent_hat = []\n",
    "        for vrej in range(len(vp_cent)):\n",
    "            v_temp = vp_cent[vrej]-skel_vecs_used[vrej]*(np.dot(vp_cent[vrej],skel_vecs_used[vrej])/np.linalg.norm(skel_vecs_used[vrej]))\n",
    "            if vrej == 6:\n",
    "                print(v_temp)\n",
    "            v_cent.append(v_temp)\n",
    "            v_cent_hat_temp = v_temp/np.linalg.norm(v_temp)\n",
    "            v_cent_hat.append(v_cent_hat_temp)\n",
    "        v_cent = np.asarray(v_cent)\n",
    "        v_cent_hat = np.asarray(v_cent_hat)\n",
    "\n",
    "        # now let's construct the frame of reference\n",
    "        # Z-direction unit vector\n",
    "        zdir = np.array([0,0,1])\n",
    "\n",
    "        # pick dir along skeleton axis as first vector in frame\n",
    "        # skeleton axis vectors of skeleton points that are closest to each Rtn4 cluster center\n",
    "        frame0_cent = vecs_hat[skel_ind_cent]\n",
    "\n",
    "        # component of z not in the direction of e0 or portion of Z-dir that is orthogonal to frame0_cent\n",
    "        frame1_cent = zdir[None,:] - frame0_cent*(frame0_cent*zdir[None,:]).sum(1)[:,None]\n",
    "\n",
    "        # orthogonal to other two vectors in the frame\n",
    "        frame2_cent = np.cross(frame0_cent,frame1_cent,axis=1)\n",
    "\n",
    "        # make them unit vectors (vector divided its magnitude)\n",
    "        frame1_cent_norm = frame1_cent/np.linalg.norm(frame1_cent,axis=1)[:,None]\n",
    "        frame2_cent_norm = frame2_cent/np.linalg.norm(frame2_cent,axis=1)[:,None]\n",
    "\n",
    "        # Determine angles_rtn between z-direction and \n",
    "        angles_cent = np.arccos((v_cent_hat*frame1_cent_norm).sum(1))  # this works because frame1_rtn is also normalized (unit length=1)\n",
    "\n",
    "        # Add layer of center points with normals set as the vectors that point to closest part of the skeleton\n",
    "        add_ds_from_Nx3(centers, pipeline, pymevis, 'cluster-centers', normals=clust_data[dt]['axis0'])\n",
    "\n",
    "        # Calculate handedness of angles\n",
    "        loc_vecs = v_cent_hat\n",
    "        hand = np.cross(skel_vecs_used,loc_vecs)\n",
    "        zhand = hand[:,2]\n",
    "        hand_bool = np.ones(len(zhand))\n",
    "        for neg in range(len(zhand)):\n",
    "            if zhand[neg] >= 0:\n",
    "                continue\n",
    "            elif zhand[neg] < 0:\n",
    "                hand_bool[neg] = hand_bool[neg] * -1\n",
    "\n",
    "        angles_cent = angles_cent * hand_bool\n",
    "        #del skel_vecs_used, loc_vecs, hand, zhand, hand_bool\n",
    "\n",
    "        # Import ExtractTableChannel Recipe\n",
    "        from PYME.recipes.localisations import ExtractTableChannel\n",
    "        recipe = pipeline.recipe\n",
    "        # Extract cluster center point\n",
    "\n",
    "        # Select simulated DataSource\n",
    "        pipeline.selectDataSource('cluster-centers')\n",
    "\n",
    "        # Calculate distance from simulated points to surface\n",
    "        from PYME.recipes.surface_fitting import DistanceToMesh\n",
    "        recipe = pipeline.recipe\n",
    "        recipe.add_modules_and_execute([DistanceToMesh(recipe, input_mesh = 'surf0', input_points = 'cluster-centers',\n",
    "                                                       output = 'centers_sdf')])\n",
    "\n",
    "        # Select simulated datasource that now has the distance_to_surf0 values associated with it\n",
    "        pipeline.selectDataSource('centers_sdf')\n",
    "\n",
    "        # Dictionary with info about what filter to use and what values\n",
    "        filt_dict_rtn = {'distance_to_surf0': [params['low_filt'],params['high_filt']]}\n",
    "\n",
    "        # Use PYME FilterTable to filter by distance to mesh (SDF) using low_filt and high_filt variables\n",
    "        from PYME.recipes.tablefilters import FilterTable\n",
    "        recipe.add_modules_and_execute([FilterTable(recipe, inputName = 'centers_sdf', filters = filt_dict_rtn, \n",
    "                                                    outputName = 'sdf_filtered_centers')])\n",
    "\n",
    "        # Select the simulate points that passed the SDF filter above\n",
    "        pipeline.selectDataSource('sdf_filtered_centers')\n",
    "\n",
    "        # Filter data\n",
    "\n",
    "        # Determine how far each cluster center is from the membrane surface so we can filter out those\n",
    "        # that are too far away\n",
    "        pipeline.selectDataSource('surf0')\n",
    "        surf_d = pipeline.selectedDataSource\n",
    "        surf_verts = surf_d._vertices['position'][surf_d._vertices['halfedge']!=-1]\n",
    "        surf_norms = surf_d._vertices['normal']\n",
    "        from scipy.spatial import cKDTree\n",
    "        tree_surf = cKDTree(surf_verts)\n",
    "        surf_dist, surf_ind = tree_surf.query(centers, k=1)\n",
    "        skel_cent_dist, skel_cent_ind = mean_tree_skeleton.query(centers, k=1)\n",
    "\n",
    "        # Create filters to filter out clusters that are too far from the surface and that are less than clust_min in size\n",
    "        sdf_filt = surf_dist < params['high_filt']\n",
    "        # min_size_filt = clust_data[dt]['count'] > params['min_size']\n",
    "        # max_size_filt = clust_data[dt]['count'] < params['max_size']\n",
    "        # size_filt = min_size_filt * max_size_filt\n",
    "        total_filt = sdf_filt# * size_filt\n",
    "\n",
    "        # Apply filter\n",
    "        clust_data[dt]['total_filt'] = total_filt\n",
    "        frame0_cent_filt = frame0_cent[total_filt]\n",
    "        frame1_cent_filt = frame1_cent_norm[total_filt]\n",
    "        frame2_cent_filt = frame2_cent_norm[total_filt]\n",
    "        clust_filt = clust_data[dt]['axis0'][total_filt]\n",
    "        centers_filt = centers[total_filt]\n",
    "        angles_cent_filt = angles_cent[total_filt]\n",
    "        v_cent_hat_filt = v_cent_hat[total_filt]\n",
    "        skel_vecs_used_filt = skel_vecs_used[total_filt]\n",
    "        ccount = clust_data[dt]['count'][total_filt]\n",
    "        gr = clust_data[dt]['gyrationRadius'][total_filt]\n",
    "        add_ds_from_Nx3(centers_filt, pipeline, pymevis, 'cluster-centers', normals=clust_filt)\n",
    "\n",
    "        from numpy import pi\n",
    "    # Simulated random orientations of clusters\n",
    "        # 4. clusters whose axes are tangent to surface in any direction (360 degrees) and positioned as real data\n",
    "\n",
    "        # Vectors can probably just point from each vertex to the closest vertex. That should be pretty random\n",
    "        # and tangent\n",
    "        # positions = cluster_center_positions\n",
    "        # vectors = tangent_to_surface_and_any_direction\\\n",
    "        # followed advice from https://stackoverflow.com/questions/71160423/how-to-sample-points-in-3d-in-python-with-origin-and-normal-vector\n",
    "\n",
    "        # parameters for this simulation\n",
    "        r = 1 # radius of points sampled in the circle on the plane (probably leave as 1 always)\n",
    "        v_num = 50 # number of vectors desired (evenly spaced around the circle)\n",
    "\n",
    "        # equations for making the vectors\n",
    "        rho = np.linspace(0, 2*np.pi, v_num)\n",
    "        x = np.cos(rho) * r\n",
    "        y = np.sin(rho) * r\n",
    "        z = np.zeros(rho.shape)\n",
    "        # positions of simulated clusters are the same as observed clusters\n",
    "        arot_pos = centers_filt\n",
    "        arot_tub = skel_vecs_used_filt\n",
    "\n",
    "        # Create vectors in a circle in the plane perpendicular to the vector that points from cluster center to skeleton\n",
    "        f_all = []\n",
    "        for bd in range(len(arot_pos)):\n",
    "            p1 = arot_pos[bd]\n",
    "            n = v_cent_hat_filt[bd] # vector normal to the plane\n",
    "            nabs = np.absolute(n)\n",
    "            indices = (-n).argsort()[:3]\n",
    "            v = np.zeros(3)\n",
    "            v[indices[1]] = -n[indices[0]]\n",
    "            v[indices[0]] = n[indices[1]]\n",
    "            v[indices[2]] = 0\n",
    "            u = np.cross(n,v)\n",
    "            #normalize vectors\n",
    "            v_norm = v/np.linalg.norm(v)\n",
    "            u_norm = u/np.linalg.norm(u)\n",
    "            f = []\n",
    "            for be in range(len(rho)):\n",
    "                circ_p = p1 + r * v_norm * np.cos(rho[be]) + r * u_norm * np.sin(rho[be])\n",
    "                circ_vec = p1-circ_p\n",
    "                f.append(circ_vec/np.linalg.norm(circ_vec))\n",
    "            f = np.asarray(f)\n",
    "            f_all.append(f)\n",
    "        f_all = np.asarray(f_all)\n",
    "\n",
    "        #del u\n",
    "        # #Add the positions with each of the simulated cluster vectors (the number of which is equal to 'rho'-1)\n",
    "        # for bf in range(len(rho)-1):\n",
    "        #     name = 'all-rots-' + str(bf)\n",
    "        #     add_ds_from_Nx3(arot_pos, pipeline, pymevis, name, normals=f_all[:,bf])\n",
    "\n",
    "        # establish z-direction\n",
    "        arot_z = zdir\n",
    "\n",
    "        # Calculate angle between simulated points/vectors and tubule axis\n",
    "        arot_tangs_all = []\n",
    "        for bi in range(len(rho)-1):\n",
    "            arot_tangs = np.zeros(len(arot_pos))\n",
    "            for bg in range(len(arot_pos)):\n",
    "                # Take arccos of the dot product to get the angle out\n",
    "                arot_tangs[bg] = np.arccos(np.dot(arot_tub[bg],f_all[:,bi][bg]))\n",
    "            # remove NaNs that occur for some reason (something about invalid value for arccos but I don't understand why)\n",
    "            # it shouldn't matter though since there are so many points anyways and they are all just going to be 0 anyways\n",
    "            # filter out the same ones for z-axis even though that shouldn't have issues, just to be consistent\n",
    "            arot_tangs = arot_tangs[~np.isnan(arot_tangs)]\n",
    "            arot_tangs_all.append(arot_tangs)\n",
    "        arot_tangs_all = np.asarray(arot_tangs_all)\n",
    "        flat_arot_tangs = [element for sublist in arot_tangs_all for element in sublist]\n",
    "        flat_arot_tangs = np.asarray(flat_arot_tangs)\n",
    "        # convert from radians to degrees\n",
    "        arot_tdeg = flat_arot_tangs * (180/pi)\n",
    "    # End of random orientation simulations\n",
    "\n",
    "    ################### Calculate angle between projected Rtn4 cluster major axis and its component orthogonal to the skeleton axis ###################\n",
    "\n",
    "    ##### New analysis using planes on the surface tangent to the skeleton and handedness\n",
    "        alpha = np.zeros(len(clust_filt))\n",
    "        psi = np.zeros(len(clust_filt))\n",
    "        sim_alpha = np.zeros(len(clust_filt)*v_num)\n",
    "        sim_psi = np.zeros(len(clust_filt)*v_num)\n",
    "        for obs in range(len(clust_filt)):\n",
    "            alpha[obs], psi[obs] = cluster_angles(v_cent_hat_filt[obs], clust_filt[obs], skel_vecs_used_filt[obs])\n",
    "            for sims in range(v_num):\n",
    "                sim_alpha[((obs*v_num)+sims)], sim_psi[((obs*v_num)+sims)] = cluster_angles(v_cent_hat_filt[obs], \n",
    "                                                                                            f_all[obs][sims],\n",
    "                                                                                            skel_vecs_used_filt[obs])\n",
    "\n",
    "        # Save a dictionary of counts, bins, and angles to cluster centers to combine experimental results later\n",
    "        cluster_output = {'center_angles': angles_cent_filt, 'alpha': alpha, 'sim_alpha': sim_alpha,\n",
    "                          'psi': psi, 'sim_psi': sim_psi, \"count\": clust_data[dt]['count'][total_filt], \"x\": clust_data[dt]['x'], \n",
    "                          \"y\": clust_data[dt]['y'][total_filt], \"z\": clust_data[dt]['z'][total_filt], \n",
    "                          \"gyrationRadius\": clust_data[dt]['gyrationRadius'][total_filt], \n",
    "                          \"median_abs_deviation\": clust_data[dt]['median_abs_deviation'][total_filt], \n",
    "                          \"sigma0\": clust_data[dt]['sigma0'][total_filt], \"sigma1\": clust_data[dt]['sigma1'][total_filt],\n",
    "                          \"sigma2\": clust_data[dt]['sigma2'][total_filt], \"sigma_x\": clust_data[dt]['sigma_x'][total_filt], \n",
    "                          \"sigma_y\": clust_data[dt]['sigma_y'][total_filt], \"sigma_z\": clust_data[dt]['sigma_z'][total_filt], \n",
    "                          \"anisotropy\": clust_data[dt]['anisotropy'][total_filt], \"theta\": clust_data[dt]['theta'][total_filt], \n",
    "                          \"phi\": clust_data[dt]['phi'][total_filt], 'dbscanID': l_data[:,30]}\n",
    "        cluster_output_all.append(cluster_output)\n",
    "                \n",
    "    ##### Simulating cluster shapes data \n",
    "        # generate positions using uniform distribution\n",
    "        rng = np.random.default_rng()\n",
    "        uni = rng.uniform(0,2*np.pi,10)\n",
    "\n",
    "\n",
    "        cs_points_sep, cs_points_tog = tan_circ_sim(rad=gr, count=ccount,\n",
    "                                                    loc_prec=np.average(clust_data[dt]['loc_prec']),\n",
    "                                                    rg_factor=1, centers=centers_filt, surf_verts=surf_verts,\n",
    "                                                   skeleton=skeleton,vecs_hat=vecs_hat)\n",
    "\n",
    "        ls_points_sep, ls_points_tog = tan_line_sim(rad=gr, count=ccount,\n",
    "                                                    loc_prec=np.average(clust_data[dt]['loc_prec']),\n",
    "                                                    rg_factor=1, centers=centers_filt, surf_verts=surf_verts,\n",
    "                                                   skeleton=skeleton,vecs_hat=vecs_hat)\n",
    "\n",
    "        adj_points_sep, adj_points_tog = tan_line_sim(rad=gr, count=ccount,\n",
    "                                                    loc_prec=np.average(clust_data[dt]['loc_prec']), \n",
    "                                                    rg_factor=rg_factor, centers=centers_filt, surf_verts=surf_verts,\n",
    "                                                   skeleton=skeleton,vecs_hat=vecs_hat)\n",
    "\n",
    "        # Measure the 3D information of each cluster and add it to a single dictionary for each simulation\n",
    "\n",
    "        from PYME.Analysis.points import cluster_morphology\n",
    "\n",
    "        base = np.zeros(len(ccount))\n",
    "        axes = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "\n",
    "        circ_clusts = {\"count\": base, \"x\": base, \"y\": base, \"z\": base, \"gyrationRadius\": base, \n",
    "                    \"median_abs_deviation\": base, \"sigma0\": base, \"sigma1\": base, \"sigma2\": base, \n",
    "                    \"sigma_x\": base, \"sigma_y\": base, \"sigma_z\": base, \"anisotropy\": base, \n",
    "                    \"theta\": base, \"phi\": base, \"axis0\": axes, \"axis1\": axes, \"axis2\": axes}\n",
    "\n",
    "        line_clusts = {\"count\": base, \"x\": base, \"y\": base, \"z\": base, \"gyrationRadius\": base, \n",
    "                    \"median_abs_deviation\": base, \"sigma0\": base, \"sigma1\": base, \"sigma2\": base, \n",
    "                    \"sigma_x\": base, \"sigma_y\": base, \"sigma_z\": base, \"anisotropy\": base, \n",
    "                    \"theta\": base, \"phi\": base, \"axis0\": axes, \"axis1\": axes, \"axis2\": axes}\n",
    "        \n",
    "        adj_clusts = {\"anisotropy\": base}\n",
    "\n",
    "        ### Measure simulated clusters using measure_3d\n",
    "\n",
    "        # initialize variables\n",
    "        circ_count = np.zeros(len(ccount))\n",
    "        circ_x = np.zeros(len(ccount))\n",
    "        circ_y = np.zeros(len(ccount))\n",
    "        circ_z = np.zeros(len(ccount))\n",
    "        circ_gR = np.zeros(len(ccount))\n",
    "        circ_mad = np.zeros(len(ccount))\n",
    "        circ_sigma0 = np.zeros(len(ccount))\n",
    "        circ_sigma1 = np.zeros(len(ccount))\n",
    "        circ_sigma2 = np.zeros(len(ccount))\n",
    "        circ_sigmax = np.zeros(len(ccount))\n",
    "        circ_sigmay = np.zeros(len(ccount))\n",
    "        circ_sigmaz = np.zeros(len(ccount))\n",
    "        circ_anisotropy = np.zeros(len(ccount))\n",
    "        circ_theta = np.zeros(len(ccount))\n",
    "        circ_phi = np.zeros(len(ccount))\n",
    "        circ_axis0 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "        circ_axis1 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "        circ_axis2 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "\n",
    "        line_count = np.zeros(len(ccount))\n",
    "        line_x = np.zeros(len(ccount))\n",
    "        line_y = np.zeros(len(ccount))\n",
    "        line_z = np.zeros(len(ccount))\n",
    "        line_gR = np.zeros(len(ccount))\n",
    "        line_mad = np.zeros(len(ccount))\n",
    "        line_sigma0 = np.zeros(len(ccount))\n",
    "        line_sigma1 = np.zeros(len(ccount))\n",
    "        line_sigma2 = np.zeros(len(ccount))\n",
    "        line_sigmax = np.zeros(len(ccount))\n",
    "        line_sigmay = np.zeros(len(ccount))\n",
    "        line_sigmaz = np.zeros(len(ccount))\n",
    "        line_anisotropy = np.zeros(len(ccount))\n",
    "        line_theta = np.zeros(len(ccount))\n",
    "        line_phi = np.zeros(len(ccount))\n",
    "        line_axis0 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "        line_axis1 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "        line_axis2 = np.zeros((np.shape(clust_data[dt]['axis0'])))\n",
    "        \n",
    "        adj_anisotropy = np.zeros(len(ccount))\n",
    "\n",
    "        for id in range(len(ccount)):\n",
    "        # measure circular simulated clusters and record info in circ_clusts dictionary\n",
    "            circ_meas = cluster_morphology.measure_3d(x=cs_points_sep[id][:,0],y=cs_points_sep[id][:,1],\n",
    "                                                      z=cs_points_sep[id][:,2])\n",
    "\n",
    "            circ_count[id] = circ_meas['count'][0]\n",
    "            circ_x[id] = circ_meas['x'][0]\n",
    "            circ_y[id] = circ_meas['y'][0]\n",
    "            circ_z[id] = circ_meas['z'][0]\n",
    "            circ_gR[id] = circ_meas['gyrationRadius'][0]\n",
    "            circ_mad[id] = circ_meas['median_abs_deviation'][0]\n",
    "            circ_sigma0[id] = circ_meas['sigma0'][0]\n",
    "            circ_sigma1[id] = circ_meas['sigma1'][0]\n",
    "            circ_sigma2[id] = circ_meas['sigma2'][0]\n",
    "            circ_sigmax[id] = circ_meas['sigma_x'][0]\n",
    "            circ_sigmay[id] = circ_meas['sigma_y'][0]\n",
    "            circ_sigmaz[id] = circ_meas['sigma_z'][0]\n",
    "            circ_anisotropy[id] = circ_meas['anisotropy'][0]\n",
    "            circ_theta[id] = circ_meas['theta'][0]\n",
    "            circ_phi[id] = circ_meas['phi'][0]\n",
    "            circ_axis0[id] = circ_meas['axis0'][0]\n",
    "            circ_axis1[id] = circ_meas['axis1'][0]\n",
    "            circ_axis2[id] = circ_meas['axis2'][0]\n",
    "\n",
    "        # Measure linear simulated clusters and record info in line_clusts dictionary\n",
    "            line_meas = cluster_morphology.measure_3d(x=ls_points_sep[id][:,0],y=ls_points_sep[id][:,1],\n",
    "                                                      z=ls_points_sep[id][:,2])\n",
    "\n",
    "            line_count[id] = line_meas['count'][0]\n",
    "            line_x[id] = line_meas['x'][0]\n",
    "            line_y[id] = line_meas['y'][0]\n",
    "            line_z[id] = line_meas['z'][0]\n",
    "            line_gR[id] = line_meas['gyrationRadius'][0]\n",
    "            line_mad[id] = line_meas['median_abs_deviation'][0]\n",
    "            line_sigma0[id] = line_meas['sigma0'][0]\n",
    "            line_sigma1[id] = line_meas['sigma1'][0]\n",
    "            line_sigma2[id] = line_meas['sigma2'][0]\n",
    "            line_sigmax[id] = line_meas['sigma_x'][0]\n",
    "            line_sigmay[id] = line_meas['sigma_y'][0]\n",
    "            line_sigmaz[id] = line_meas['sigma_z'][0]\n",
    "            line_anisotropy[id] = line_meas['anisotropy'][0]\n",
    "            line_theta[id] = line_meas['theta'][0]\n",
    "            line_phi[id] = line_meas['phi'][0]\n",
    "            line_axis0[id] = line_meas['axis0'][0]\n",
    "            line_axis1[id] = line_meas['axis1'][0]\n",
    "            line_axis2[id] = line_meas['axis2'][0]\n",
    "            \n",
    "        # Measure adjusted linear simulated clusters and record anisotropy\n",
    "            adj_meas = cluster_morphology.measure_3d(x=adj_points_sep[id][:,0],y=adj_points_sep[id][:,1],\n",
    "                                                      z=adj_points_sep[id][:,2])\n",
    "            \n",
    "            adj_anisotropy[id] = adj_meas['anisotropy'][0]\n",
    "            \n",
    "        circ_clusts['count'] = np.asarray(circ_count)\n",
    "        circ_clusts['x'] = np.asarray(circ_x)\n",
    "        circ_clusts['y'] = np.asarray(circ_y)\n",
    "        circ_clusts['z'] = np.asarray(circ_z)\n",
    "        circ_clusts['gyrationRadius'] = np.asarray(circ_gR)\n",
    "        circ_clusts['median_abs_deviation'] = np.asarray(circ_mad)\n",
    "        circ_clusts['sigma0'] = np.asarray(circ_sigma0)\n",
    "        circ_clusts['sigma1'] = np.asarray(circ_sigma1)\n",
    "        circ_clusts['sigma2'] = np.asarray(circ_sigma2)\n",
    "        circ_clusts['sigma_x'] = np.asarray(circ_sigmax)\n",
    "        circ_clusts['sigma_y'] = np.asarray(circ_sigmay)\n",
    "        circ_clusts['sigma_z'] = np.asarray(circ_sigmaz)\n",
    "        circ_clusts['anisotropy'] = np.asarray(circ_anisotropy)\n",
    "        circ_clusts['theta'] = np.asarray(circ_theta)\n",
    "        circ_clusts['phi'] = np.asarray(circ_phi)\n",
    "        circ_clusts['axis0'] = np.asarray(circ_axis0)\n",
    "        circ_clusts['axis1'] = np.asarray(circ_axis1)\n",
    "        circ_clusts['axis2'] = np.asarray(circ_axis2)\n",
    "\n",
    "        line_clusts['count'] = np.asarray(line_count)\n",
    "        line_clusts['x'] = np.asarray(line_x)\n",
    "        line_clusts['y'] = np.asarray(line_y)\n",
    "        line_clusts['z'] = np.asarray(line_z)\n",
    "        line_clusts['gyrationRadius'] = np.asarray(line_gR)\n",
    "        line_clusts['median_abs_deviation'] = np.asarray(line_mad)\n",
    "        line_clusts['sigma0'] = np.asarray(line_sigma0)\n",
    "        line_clusts['sigma1'] = np.asarray(line_sigma1)\n",
    "        line_clusts['sigma2'] = np.asarray(line_sigma2)\n",
    "        line_clusts['sigma_x'] = np.asarray(line_sigmax)\n",
    "        line_clusts['sigma_y'] = np.asarray(line_sigmay)\n",
    "        line_clusts['sigma_z'] = np.asarray(line_sigmaz)\n",
    "        line_clusts['anisotropy'] = np.asarray(line_anisotropy)\n",
    "        line_clusts['theta'] = np.asarray(line_theta)\n",
    "        line_clusts['phi'] = np.asarray(line_phi)\n",
    "        line_clusts['axis0'] = np.asarray(line_axis0)\n",
    "        line_clusts['axis1'] = np.asarray(line_axis1)\n",
    "        line_clusts['axis2'] = np.asarray(line_axis2)\n",
    "        \n",
    "        adj_clusts['anisotropy'] = np.asarray(adj_anisotropy)\n",
    "\n",
    "        circ_sims_all.append(circ_clusts)\n",
    "        line_sims_all.append(line_clusts)\n",
    "        adj_sims_all.append(adj_clusts)\n",
    "\n",
    "    save_circ_sims = params['savedir'] + '\\\\circular_sims_' + params['ver'] + '.npy'\n",
    "    np.save(save_circ_sims, circ_sims_all, allow_pickle=True)\n",
    "    \n",
    "    save_line_sims = params['savedir'] + '\\\\linear_sims_' + params['ver'] + '.npy'\n",
    "    np.save(save_line_sims, line_sims_all, allow_pickle=True)\n",
    "    \n",
    "    save_adj_sims = params['savedir'] + '\\\\adj_sims_' + params['ver'] + '.npy'\n",
    "    np.save(save_adj_sims, adj_sims_all, allow_pickle=True)\n",
    "    \n",
    "    save_output_file = params['savedir'] + '\\\\cluster_output_' + params['ver'] + '.npy'\n",
    "    np.save(save_output_file, cluster_output_all, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf90ffb-9db5-4307-b77f-c507c291b9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_analysis_V1(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23b542-6e3a-4014-a163-e4bc3e2de969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyme",
   "language": "python",
   "name": "pyme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
